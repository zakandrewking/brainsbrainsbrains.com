OpenAI has a new model. But it's not GPT-5 (please, please, we just like how it
feels increment a number!). It's called "o1", and it's a GPT that's fine-tuned
for chain-of-thought reasoning.

On one hand, chain-of-thought prompting has been possible all along with GPT-3+,
and is widely used. Heck, it's on site called
[LearnPrompting.org](https://learnprompting.org/docs/intermediate/chain_of_thought).

But OpenAI has taken things a step further by (1) training this model to use
chain-of-thought all the time and, (2) more importantly, hiding the
chain-of-thought from users. So we don't see a summary of the reasoning steps
but not the gory details. This might provide some protection from other models
training on o1 output, but it's a bummer.

I don't have access to op1 via API, but I do have it in ChatGPT. And it has no
problem with the classic strawberry problem.

<img src="/chain-of-thought/strawberry.png" style={{ width: "100%", maxWidth:
"550px", marginBottom: "15px" }} />

So, at least we have that. I like to challenge these models to _teach me
something_; let's try that.
